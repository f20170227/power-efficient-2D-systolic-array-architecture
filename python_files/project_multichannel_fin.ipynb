{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "radical-fifty",
    "outputId": "f0e9ccce-e06c-4728-8348-5dfa79c6eeb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "0\n",
      "3\n",
      "6\n",
      "7\n",
      "10\n",
      "13\n",
      "14\n",
      "17\n",
      "20\n",
      "23\n",
      "24\n",
      "27\n",
      "29\n",
      "32\n",
      "33\n",
      "36\n",
      "39\n",
      "42\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:01<00:00, 92383323.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [35, 50]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ypLdskKyd4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ypLdskKyd4d",
    "outputId": "bafade84-8586-45d1-d57a-77616192bb60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/782]\tTime 3.318 (3.318)\tData 0.428 (0.428)\tLoss 2.5092 (2.5092)\tPrec 12.500% (12.500%)\n",
      "Epoch: [0][100/782]\tTime 0.047 (0.073)\tData 0.003 (0.007)\tLoss 2.3547 (3.4738)\tPrec 14.062% (11.897%)\n",
      "Epoch: [0][200/782]\tTime 0.039 (0.057)\tData 0.001 (0.005)\tLoss 2.1469 (2.8323)\tPrec 21.875% (14.327%)\n",
      "Epoch: [0][300/782]\tTime 0.042 (0.055)\tData 0.008 (0.005)\tLoss 2.1293 (2.5861)\tPrec 18.750% (16.331%)\n",
      "Epoch: [0][400/782]\tTime 0.040 (0.051)\tData 0.002 (0.004)\tLoss 1.8556 (2.4440)\tPrec 28.125% (17.507%)\n",
      "Epoch: [0][500/782]\tTime 0.044 (0.049)\tData 0.002 (0.004)\tLoss 1.9513 (2.3523)\tPrec 23.438% (18.491%)\n",
      "Epoch: [0][600/782]\tTime 0.038 (0.049)\tData 0.002 (0.004)\tLoss 1.9548 (2.2859)\tPrec 23.438% (19.215%)\n",
      "Epoch: [0][700/782]\tTime 0.038 (0.048)\tData 0.002 (0.004)\tLoss 1.9229 (2.2351)\tPrec 18.750% (19.755%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.120 (0.120)\tLoss 1.8119 (1.8119)\tPrec 32.812% (32.812%)\n",
      "Test: [100/157]\tTime 0.031 (0.025)\tLoss 1.8940 (1.8549)\tPrec 28.125% (26.284%)\n",
      " * Prec 26.460% \n",
      "best acc: 26.460000\n",
      "Epoch: [1][0/782]\tTime 0.302 (0.302)\tData 0.209 (0.209)\tLoss 1.8012 (1.8012)\tPrec 18.750% (18.750%)\n",
      "Epoch: [1][100/782]\tTime 0.041 (0.044)\tData 0.002 (0.005)\tLoss 1.9582 (1.8974)\tPrec 20.312% (24.861%)\n",
      "Epoch: [1][200/782]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 1.7427 (1.8900)\tPrec 28.125% (25.878%)\n",
      "Epoch: [1][300/782]\tTime 0.071 (0.046)\tData 0.015 (0.004)\tLoss 1.8658 (1.8746)\tPrec 32.812% (26.869%)\n",
      "Epoch: [1][400/782]\tTime 0.044 (0.045)\tData 0.011 (0.004)\tLoss 1.9021 (1.8681)\tPrec 31.250% (27.081%)\n",
      "Epoch: [1][500/782]\tTime 0.039 (0.044)\tData 0.002 (0.004)\tLoss 1.6877 (1.8613)\tPrec 25.000% (27.236%)\n",
      "Epoch: [1][600/782]\tTime 0.064 (0.045)\tData 0.007 (0.004)\tLoss 1.8794 (1.8539)\tPrec 15.625% (27.561%)\n",
      "Epoch: [1][700/782]\tTime 0.041 (0.045)\tData 0.003 (0.004)\tLoss 2.1288 (1.8454)\tPrec 31.250% (28.002%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.120 (0.120)\tLoss 1.7035 (1.7035)\tPrec 37.500% (37.500%)\n",
      "Test: [100/157]\tTime 0.032 (0.023)\tLoss 1.8007 (1.8145)\tPrec 31.250% (30.229%)\n",
      " * Prec 30.330% \n",
      "best acc: 30.330000\n",
      "Epoch: [2][0/782]\tTime 0.274 (0.274)\tData 0.215 (0.215)\tLoss 1.7442 (1.7442)\tPrec 37.500% (37.500%)\n",
      "Epoch: [2][100/782]\tTime 0.041 (0.050)\tData 0.002 (0.004)\tLoss 1.6495 (1.7386)\tPrec 34.375% (32.147%)\n",
      "Epoch: [2][200/782]\tTime 0.039 (0.045)\tData 0.001 (0.003)\tLoss 1.5306 (1.7269)\tPrec 43.750% (32.781%)\n",
      "Epoch: [2][300/782]\tTime 0.046 (0.044)\tData 0.002 (0.003)\tLoss 1.8411 (1.7218)\tPrec 32.812% (33.534%)\n",
      "Epoch: [2][400/782]\tTime 0.063 (0.051)\tData 0.002 (0.004)\tLoss 1.4959 (1.7094)\tPrec 42.188% (33.896%)\n",
      "Epoch: [2][500/782]\tTime 0.037 (0.049)\tData 0.002 (0.004)\tLoss 1.7515 (1.6962)\tPrec 34.375% (34.291%)\n",
      "Epoch: [2][600/782]\tTime 0.072 (0.048)\tData 0.002 (0.004)\tLoss 1.7650 (1.6825)\tPrec 32.812% (34.814%)\n",
      "Epoch: [2][700/782]\tTime 0.040 (0.048)\tData 0.003 (0.004)\tLoss 1.6598 (1.6715)\tPrec 29.688% (35.367%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.122 (0.122)\tLoss 1.5126 (1.5126)\tPrec 43.750% (43.750%)\n",
      "Test: [100/157]\tTime 0.022 (0.023)\tLoss 1.5994 (1.5823)\tPrec 42.188% (39.604%)\n",
      " * Prec 40.020% \n",
      "best acc: 40.020000\n",
      "Epoch: [3][0/782]\tTime 0.163 (0.163)\tData 0.114 (0.114)\tLoss 1.4367 (1.4367)\tPrec 42.188% (42.188%)\n",
      "Epoch: [3][100/782]\tTime 0.042 (0.053)\tData 0.002 (0.006)\tLoss 1.5462 (1.5556)\tPrec 46.875% (40.548%)\n",
      "Epoch: [3][200/782]\tTime 0.038 (0.047)\tData 0.004 (0.004)\tLoss 1.6291 (1.5440)\tPrec 35.938% (41.620%)\n",
      "Epoch: [3][300/782]\tTime 0.038 (0.045)\tData 0.002 (0.004)\tLoss 1.2109 (1.5247)\tPrec 54.688% (42.380%)\n",
      "Epoch: [3][400/782]\tTime 0.039 (0.046)\tData 0.006 (0.004)\tLoss 1.5768 (1.5131)\tPrec 51.562% (42.943%)\n",
      "Epoch: [3][500/782]\tTime 0.040 (0.045)\tData 0.002 (0.004)\tLoss 1.2234 (1.4989)\tPrec 48.438% (43.494%)\n",
      "Epoch: [3][600/782]\tTime 0.038 (0.044)\tData 0.006 (0.004)\tLoss 1.3485 (1.4888)\tPrec 40.625% (44.080%)\n",
      "Epoch: [3][700/782]\tTime 0.033 (0.046)\tData 0.000 (0.004)\tLoss 1.2781 (1.4765)\tPrec 54.688% (44.688%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.129 (0.129)\tLoss 1.5466 (1.5466)\tPrec 40.625% (40.625%)\n",
      "Test: [100/157]\tTime 0.028 (0.023)\tLoss 1.4762 (1.4129)\tPrec 45.312% (46.535%)\n",
      " * Prec 46.950% \n",
      "best acc: 46.950000\n",
      "Epoch: [4][0/782]\tTime 0.177 (0.177)\tData 0.106 (0.106)\tLoss 1.3703 (1.3703)\tPrec 53.125% (53.125%)\n",
      "Epoch: [4][100/782]\tTime 0.069 (0.051)\tData 0.015 (0.006)\tLoss 1.3603 (1.3531)\tPrec 48.438% (50.387%)\n",
      "Epoch: [4][200/782]\tTime 0.038 (0.047)\tData 0.003 (0.004)\tLoss 1.0964 (1.3483)\tPrec 53.125% (50.684%)\n",
      "Epoch: [4][300/782]\tTime 0.040 (0.045)\tData 0.002 (0.004)\tLoss 1.4992 (1.3350)\tPrec 50.000% (51.313%)\n",
      "Epoch: [4][400/782]\tTime 0.060 (0.046)\tData 0.001 (0.004)\tLoss 1.3073 (1.3133)\tPrec 46.875% (52.159%)\n",
      "Epoch: [4][500/782]\tTime 0.040 (0.046)\tData 0.002 (0.004)\tLoss 1.0508 (1.2975)\tPrec 54.688% (52.732%)\n",
      "Epoch: [4][600/782]\tTime 0.040 (0.045)\tData 0.006 (0.004)\tLoss 1.1332 (1.2815)\tPrec 60.938% (53.393%)\n",
      "Epoch: [4][700/782]\tTime 0.057 (0.045)\tData 0.007 (0.004)\tLoss 1.4224 (1.2702)\tPrec 51.562% (53.910%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.128 (0.128)\tLoss 1.1753 (1.1753)\tPrec 59.375% (59.375%)\n",
      "Test: [100/157]\tTime 0.023 (0.024)\tLoss 1.2097 (1.1431)\tPrec 50.000% (59.251%)\n",
      " * Prec 59.050% \n",
      "best acc: 59.050000\n",
      "Epoch: [5][0/782]\tTime 0.173 (0.173)\tData 0.107 (0.107)\tLoss 0.9975 (0.9975)\tPrec 62.500% (62.500%)\n",
      "Epoch: [5][100/782]\tTime 0.098 (0.045)\tData 0.002 (0.004)\tLoss 1.2126 (1.1366)\tPrec 54.688% (58.710%)\n",
      "Epoch: [5][200/782]\tTime 0.039 (0.047)\tData 0.002 (0.004)\tLoss 1.1279 (1.1211)\tPrec 54.688% (59.453%)\n",
      "Epoch: [5][300/782]\tTime 0.040 (0.046)\tData 0.002 (0.004)\tLoss 0.9246 (1.1114)\tPrec 62.500% (59.821%)\n",
      "Epoch: [5][400/782]\tTime 0.080 (0.046)\tData 0.002 (0.004)\tLoss 1.2562 (1.1076)\tPrec 59.375% (60.088%)\n",
      "Epoch: [5][500/782]\tTime 0.040 (0.046)\tData 0.001 (0.004)\tLoss 0.9172 (1.1022)\tPrec 67.188% (60.520%)\n",
      "Epoch: [5][600/782]\tTime 0.045 (0.045)\tData 0.004 (0.004)\tLoss 1.0520 (1.0883)\tPrec 64.062% (61.249%)\n",
      "Epoch: [5][700/782]\tTime 0.057 (0.045)\tData 0.002 (0.004)\tLoss 0.9697 (1.0797)\tPrec 68.750% (61.584%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.131 (0.131)\tLoss 1.0133 (1.0133)\tPrec 67.188% (67.188%)\n",
      "Test: [100/157]\tTime 0.039 (0.023)\tLoss 1.0363 (1.0119)\tPrec 64.062% (64.991%)\n",
      " * Prec 64.150% \n",
      "best acc: 64.150000\n",
      "Epoch: [6][0/782]\tTime 0.166 (0.166)\tData 0.123 (0.123)\tLoss 0.9101 (0.9101)\tPrec 70.312% (70.312%)\n",
      "Epoch: [6][100/782]\tTime 0.043 (0.043)\tData 0.006 (0.004)\tLoss 0.8972 (0.9919)\tPrec 70.312% (65.114%)\n",
      "Epoch: [6][200/782]\tTime 0.039 (0.047)\tData 0.001 (0.004)\tLoss 1.1376 (0.9646)\tPrec 54.688% (65.905%)\n",
      "Epoch: [6][300/782]\tTime 0.039 (0.045)\tData 0.002 (0.004)\tLoss 0.8930 (0.9618)\tPrec 65.625% (65.911%)\n",
      "Epoch: [6][400/782]\tTime 0.046 (0.044)\tData 0.008 (0.004)\tLoss 0.7922 (0.9533)\tPrec 73.438% (66.350%)\n",
      "Epoch: [6][500/782]\tTime 0.038 (0.045)\tData 0.002 (0.004)\tLoss 1.1687 (0.9399)\tPrec 59.375% (66.807%)\n",
      "Epoch: [6][600/782]\tTime 0.041 (0.045)\tData 0.006 (0.003)\tLoss 0.7567 (0.9336)\tPrec 71.875% (67.141%)\n",
      "Epoch: [6][700/782]\tTime 0.041 (0.044)\tData 0.001 (0.003)\tLoss 0.8759 (0.9290)\tPrec 67.188% (67.359%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.120 (0.120)\tLoss 0.8557 (0.8557)\tPrec 65.625% (65.625%)\n",
      "Test: [100/157]\tTime 0.023 (0.023)\tLoss 0.8925 (0.8553)\tPrec 67.188% (69.307%)\n",
      " * Prec 69.040% \n",
      "best acc: 69.040000\n",
      "Epoch: [7][0/782]\tTime 0.175 (0.175)\tData 0.105 (0.105)\tLoss 0.9006 (0.9006)\tPrec 68.750% (68.750%)\n",
      "Epoch: [7][100/782]\tTime 0.040 (0.043)\tData 0.002 (0.003)\tLoss 0.7168 (0.8773)\tPrec 68.750% (68.920%)\n",
      "Epoch: [7][200/782]\tTime 0.040 (0.047)\tData 0.003 (0.004)\tLoss 0.8043 (0.8562)\tPrec 68.750% (69.792%)\n",
      "Epoch: [7][300/782]\tTime 0.039 (0.045)\tData 0.003 (0.003)\tLoss 0.9060 (0.8496)\tPrec 59.375% (70.126%)\n",
      "Epoch: [7][400/782]\tTime 0.039 (0.044)\tData 0.003 (0.003)\tLoss 0.6146 (0.8434)\tPrec 82.812% (70.211%)\n",
      "Epoch: [7][500/782]\tTime 0.053 (0.045)\tData 0.002 (0.004)\tLoss 0.8432 (0.8389)\tPrec 70.312% (70.366%)\n",
      "Epoch: [7][600/782]\tTime 0.040 (0.045)\tData 0.001 (0.003)\tLoss 0.7703 (0.8345)\tPrec 70.312% (70.572%)\n",
      "Epoch: [7][700/782]\tTime 0.040 (0.044)\tData 0.002 (0.003)\tLoss 0.7092 (0.8305)\tPrec 75.000% (70.792%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.188 (0.188)\tLoss 0.8819 (0.8819)\tPrec 67.188% (67.188%)\n",
      "Test: [100/157]\tTime 0.022 (0.028)\tLoss 0.8806 (0.8441)\tPrec 73.438% (71.272%)\n",
      " * Prec 71.310% \n",
      "best acc: 71.310000\n",
      "Epoch: [8][0/782]\tTime 0.184 (0.184)\tData 0.112 (0.112)\tLoss 0.9104 (0.9104)\tPrec 64.062% (64.062%)\n",
      "Epoch: [8][100/782]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.6526 (0.7810)\tPrec 71.875% (73.283%)\n",
      "Epoch: [8][200/782]\tTime 0.054 (0.044)\tData 0.004 (0.004)\tLoss 1.0389 (0.7714)\tPrec 68.750% (73.647%)\n",
      "Epoch: [8][300/782]\tTime 0.040 (0.045)\tData 0.002 (0.004)\tLoss 0.9715 (0.7644)\tPrec 60.938% (73.604%)\n",
      "Epoch: [8][400/782]\tTime 0.042 (0.044)\tData 0.002 (0.003)\tLoss 0.7553 (0.7617)\tPrec 73.438% (73.679%)\n",
      "Epoch: [8][500/782]\tTime 0.072 (0.044)\tData 0.000 (0.003)\tLoss 0.6245 (0.7600)\tPrec 75.000% (73.706%)\n",
      "Epoch: [8][600/782]\tTime 0.040 (0.045)\tData 0.004 (0.003)\tLoss 0.5800 (0.7581)\tPrec 78.125% (73.770%)\n",
      "Epoch: [8][700/782]\tTime 0.041 (0.044)\tData 0.001 (0.003)\tLoss 0.4769 (0.7576)\tPrec 84.375% (73.774%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.145 (0.145)\tLoss 1.0124 (1.0124)\tPrec 62.500% (62.500%)\n",
      "Test: [100/157]\tTime 0.038 (0.034)\tLoss 0.8089 (0.9475)\tPrec 70.312% (68.209%)\n",
      " * Prec 68.230% \n",
      "best acc: 71.310000\n",
      "Epoch: [9][0/782]\tTime 0.198 (0.198)\tData 0.124 (0.124)\tLoss 0.7797 (0.7797)\tPrec 68.750% (68.750%)\n",
      "Epoch: [9][100/782]\tTime 0.039 (0.043)\tData 0.002 (0.004)\tLoss 1.0105 (0.7158)\tPrec 62.500% (75.155%)\n",
      "Epoch: [9][200/782]\tTime 0.064 (0.043)\tData 0.010 (0.004)\tLoss 0.6505 (0.7069)\tPrec 76.562% (75.606%)\n",
      "Epoch: [9][300/782]\tTime 0.041 (0.046)\tData 0.001 (0.004)\tLoss 0.6844 (0.7062)\tPrec 75.000% (75.524%)\n",
      "Epoch: [9][400/782]\tTime 0.040 (0.044)\tData 0.002 (0.003)\tLoss 0.8491 (0.7096)\tPrec 70.312% (75.429%)\n",
      "Epoch: [9][500/782]\tTime 0.039 (0.044)\tData 0.002 (0.003)\tLoss 0.6082 (0.7092)\tPrec 76.562% (75.527%)\n",
      "Epoch: [9][600/782]\tTime 0.039 (0.045)\tData 0.003 (0.003)\tLoss 0.6655 (0.7084)\tPrec 79.688% (75.692%)\n",
      "Epoch: [9][700/782]\tTime 0.039 (0.044)\tData 0.001 (0.003)\tLoss 0.7381 (0.7032)\tPrec 75.000% (75.869%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.124 (0.124)\tLoss 0.9335 (0.9335)\tPrec 73.438% (73.438%)\n",
      "Test: [100/157]\tTime 0.044 (0.030)\tLoss 0.8122 (0.7353)\tPrec 71.875% (74.660%)\n",
      " * Prec 74.900% \n",
      "best acc: 74.900000\n",
      "Epoch: [10][0/782]\tTime 0.176 (0.176)\tData 0.130 (0.130)\tLoss 0.7985 (0.7985)\tPrec 67.188% (67.188%)\n",
      "Epoch: [10][100/782]\tTime 0.040 (0.042)\tData 0.001 (0.004)\tLoss 0.8156 (0.6541)\tPrec 70.312% (77.181%)\n",
      "Epoch: [10][200/782]\tTime 0.040 (0.041)\tData 0.002 (0.003)\tLoss 0.5385 (0.6501)\tPrec 79.688% (77.806%)\n",
      "Epoch: [10][300/782]\tTime 0.040 (0.045)\tData 0.004 (0.004)\tLoss 0.7950 (0.6491)\tPrec 71.875% (77.933%)\n",
      "Epoch: [10][400/782]\tTime 0.039 (0.044)\tData 0.002 (0.003)\tLoss 0.6268 (0.6479)\tPrec 79.688% (77.876%)\n",
      "Epoch: [10][500/782]\tTime 0.041 (0.043)\tData 0.002 (0.003)\tLoss 0.6335 (0.6470)\tPrec 79.688% (77.929%)\n",
      "Epoch: [10][600/782]\tTime 0.038 (0.045)\tData 0.002 (0.003)\tLoss 0.6717 (0.6501)\tPrec 78.125% (77.816%)\n",
      "Epoch: [10][700/782]\tTime 0.039 (0.044)\tData 0.002 (0.003)\tLoss 0.4605 (0.6454)\tPrec 82.812% (77.982%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.133 (0.133)\tLoss 0.7661 (0.7661)\tPrec 76.562% (76.562%)\n",
      "Test: [100/157]\tTime 0.029 (0.024)\tLoss 0.5441 (0.6608)\tPrec 81.250% (77.955%)\n",
      " * Prec 77.770% \n",
      "best acc: 77.770000\n",
      "Epoch: [11][0/782]\tTime 0.285 (0.285)\tData 0.203 (0.203)\tLoss 0.6065 (0.6065)\tPrec 73.438% (73.438%)\n",
      "Epoch: [11][100/782]\tTime 0.042 (0.045)\tData 0.001 (0.004)\tLoss 0.5393 (0.6271)\tPrec 81.250% (78.589%)\n",
      "Epoch: [11][200/782]\tTime 0.040 (0.043)\tData 0.002 (0.004)\tLoss 0.7435 (0.6062)\tPrec 73.438% (79.415%)\n",
      "Epoch: [11][300/782]\tTime 0.060 (0.045)\tData 0.002 (0.004)\tLoss 0.6202 (0.6064)\tPrec 78.125% (79.428%)\n",
      "Epoch: [11][400/782]\tTime 0.042 (0.045)\tData 0.002 (0.004)\tLoss 0.5490 (0.6088)\tPrec 85.938% (79.313%)\n",
      "Epoch: [11][500/782]\tTime 0.039 (0.044)\tData 0.001 (0.003)\tLoss 0.5871 (0.6113)\tPrec 84.375% (79.270%)\n",
      "Epoch: [11][600/782]\tTime 0.062 (0.044)\tData 0.006 (0.003)\tLoss 0.8170 (0.6100)\tPrec 73.438% (79.350%)\n",
      "Epoch: [11][700/782]\tTime 0.050 (0.044)\tData 0.002 (0.003)\tLoss 0.5116 (0.6088)\tPrec 79.688% (79.494%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.113 (0.113)\tLoss 0.8177 (0.8177)\tPrec 67.188% (67.188%)\n",
      "Test: [100/157]\tTime 0.020 (0.022)\tLoss 0.6809 (0.6657)\tPrec 73.438% (77.676%)\n",
      " * Prec 77.350% \n",
      "best acc: 77.770000\n",
      "Epoch: [12][0/782]\tTime 0.255 (0.255)\tData 0.193 (0.193)\tLoss 0.4844 (0.4844)\tPrec 84.375% (84.375%)\n",
      "Epoch: [12][100/782]\tTime 0.037 (0.051)\tData 0.002 (0.006)\tLoss 0.7719 (0.5685)\tPrec 73.438% (80.848%)\n",
      "Epoch: [12][200/782]\tTime 0.039 (0.046)\tData 0.001 (0.004)\tLoss 0.5756 (0.5626)\tPrec 81.250% (80.877%)\n",
      "Epoch: [12][300/782]\tTime 0.062 (0.045)\tData 0.002 (0.004)\tLoss 0.5461 (0.5714)\tPrec 79.688% (80.596%)\n",
      "Epoch: [12][400/782]\tTime 0.039 (0.046)\tData 0.003 (0.004)\tLoss 0.6101 (0.5734)\tPrec 82.812% (80.553%)\n",
      "Epoch: [12][500/782]\tTime 0.040 (0.045)\tData 0.002 (0.003)\tLoss 0.5944 (0.5667)\tPrec 87.500% (80.810%)\n",
      "Epoch: [12][600/782]\tTime 0.042 (0.045)\tData 0.007 (0.003)\tLoss 0.5331 (0.5653)\tPrec 84.375% (80.868%)\n",
      "Epoch: [12][700/782]\tTime 0.038 (0.045)\tData 0.002 (0.004)\tLoss 0.4899 (0.5669)\tPrec 81.250% (80.900%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.120 (0.120)\tLoss 0.8930 (0.8930)\tPrec 70.312% (70.312%)\n",
      "Test: [100/157]\tTime 0.021 (0.024)\tLoss 0.4375 (0.6247)\tPrec 85.938% (79.270%)\n",
      " * Prec 79.500% \n",
      "best acc: 79.500000\n",
      "Epoch: [13][0/782]\tTime 0.170 (0.170)\tData 0.107 (0.107)\tLoss 0.5075 (0.5075)\tPrec 84.375% (84.375%)\n",
      "Epoch: [13][100/782]\tTime 0.040 (0.053)\tData 0.002 (0.006)\tLoss 0.3798 (0.5553)\tPrec 87.500% (80.724%)\n",
      "Epoch: [13][200/782]\tTime 0.050 (0.048)\tData 0.002 (0.004)\tLoss 0.4795 (0.5597)\tPrec 82.812% (81.250%)\n",
      "Epoch: [13][300/782]\tTime 0.049 (0.051)\tData 0.005 (0.005)\tLoss 0.5292 (0.5565)\tPrec 79.688% (81.598%)\n",
      "Epoch: [13][400/782]\tTime 0.041 (0.050)\tData 0.002 (0.004)\tLoss 0.3587 (0.5472)\tPrec 89.062% (81.823%)\n",
      "Epoch: [13][500/782]\tTime 0.040 (0.048)\tData 0.001 (0.004)\tLoss 0.4445 (0.5465)\tPrec 81.250% (81.755%)\n",
      "Epoch: [13][600/782]\tTime 0.055 (0.047)\tData 0.002 (0.004)\tLoss 0.5483 (0.5423)\tPrec 81.250% (81.843%)\n",
      "Epoch: [13][700/782]\tTime 0.042 (0.048)\tData 0.006 (0.004)\tLoss 0.4976 (0.5411)\tPrec 84.375% (81.923%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.129 (0.129)\tLoss 0.6430 (0.6430)\tPrec 75.000% (75.000%)\n",
      "Test: [100/157]\tTime 0.023 (0.023)\tLoss 0.6652 (0.5724)\tPrec 76.562% (80.879%)\n",
      " * Prec 80.600% \n",
      "best acc: 80.600000\n",
      "Epoch: [14][0/782]\tTime 0.182 (0.182)\tData 0.109 (0.109)\tLoss 0.3922 (0.3922)\tPrec 84.375% (84.375%)\n",
      "Epoch: [14][100/782]\tTime 0.042 (0.052)\tData 0.002 (0.004)\tLoss 0.3652 (0.5029)\tPrec 92.188% (82.704%)\n",
      "Epoch: [14][200/782]\tTime 0.041 (0.047)\tData 0.001 (0.003)\tLoss 0.4375 (0.5111)\tPrec 89.062% (82.548%)\n",
      "Epoch: [14][300/782]\tTime 0.041 (0.045)\tData 0.007 (0.003)\tLoss 0.5654 (0.5113)\tPrec 79.688% (82.543%)\n",
      "Epoch: [14][400/782]\tTime 0.064 (0.046)\tData 0.002 (0.003)\tLoss 0.5419 (0.5127)\tPrec 81.250% (82.563%)\n",
      "Epoch: [14][500/782]\tTime 0.038 (0.046)\tData 0.001 (0.003)\tLoss 0.5217 (0.5135)\tPrec 84.375% (82.594%)\n",
      "Epoch: [14][600/782]\tTime 0.041 (0.045)\tData 0.005 (0.003)\tLoss 0.4058 (0.5130)\tPrec 90.625% (82.680%)\n",
      "Epoch: [14][700/782]\tTime 0.038 (0.046)\tData 0.002 (0.003)\tLoss 0.7259 (0.5121)\tPrec 81.250% (82.761%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.118 (0.118)\tLoss 0.6928 (0.6928)\tPrec 81.250% (81.250%)\n",
      "Test: [100/157]\tTime 0.023 (0.023)\tLoss 0.6321 (0.6854)\tPrec 85.938% (78.342%)\n",
      " * Prec 78.120% \n",
      "best acc: 80.600000\n",
      "Epoch: [15][0/782]\tTime 0.170 (0.170)\tData 0.106 (0.106)\tLoss 0.5848 (0.5848)\tPrec 79.688% (79.688%)\n",
      "Epoch: [15][100/782]\tTime 0.051 (0.051)\tData 0.002 (0.005)\tLoss 0.3784 (0.5079)\tPrec 84.375% (82.890%)\n",
      "Epoch: [15][200/782]\tTime 0.040 (0.048)\tData 0.002 (0.004)\tLoss 0.6540 (0.4988)\tPrec 76.562% (83.170%)\n",
      "Epoch: [15][300/782]\tTime 0.041 (0.046)\tData 0.001 (0.004)\tLoss 0.5523 (0.4938)\tPrec 78.125% (83.259%)\n",
      "Epoch: [15][400/782]\tTime 0.049 (0.047)\tData 0.002 (0.004)\tLoss 0.4672 (0.4943)\tPrec 79.688% (83.229%)\n",
      "Epoch: [15][500/782]\tTime 0.039 (0.046)\tData 0.002 (0.003)\tLoss 0.5948 (0.4903)\tPrec 84.375% (83.377%)\n",
      "Epoch: [15][600/782]\tTime 0.037 (0.045)\tData 0.002 (0.003)\tLoss 0.3624 (0.4889)\tPrec 87.500% (83.413%)\n",
      "Epoch: [15][700/782]\tTime 0.053 (0.046)\tData 0.003 (0.003)\tLoss 0.5202 (0.4880)\tPrec 82.812% (83.463%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.133 (0.133)\tLoss 0.5951 (0.5951)\tPrec 84.375% (84.375%)\n",
      "Test: [100/157]\tTime 0.018 (0.023)\tLoss 0.5324 (0.5204)\tPrec 75.000% (82.550%)\n",
      " * Prec 82.040% \n",
      "best acc: 82.040000\n",
      "Epoch: [16][0/782]\tTime 0.183 (0.183)\tData 0.108 (0.108)\tLoss 0.4296 (0.4296)\tPrec 87.500% (87.500%)\n",
      "Epoch: [16][100/782]\tTime 0.059 (0.049)\tData 0.010 (0.005)\tLoss 0.4650 (0.4673)\tPrec 87.500% (84.514%)\n",
      "Epoch: [16][200/782]\tTime 0.039 (0.047)\tData 0.002 (0.004)\tLoss 0.5509 (0.4756)\tPrec 84.375% (84.274%)\n",
      "Epoch: [16][300/782]\tTime 0.041 (0.045)\tData 0.005 (0.003)\tLoss 0.6364 (0.4721)\tPrec 85.938% (84.448%)\n",
      "Epoch: [16][400/782]\tTime 0.053 (0.045)\tData 0.002 (0.003)\tLoss 0.5417 (0.4685)\tPrec 84.375% (84.500%)\n",
      "Epoch: [16][500/782]\tTime 0.039 (0.045)\tData 0.002 (0.003)\tLoss 0.5606 (0.4656)\tPrec 81.250% (84.419%)\n",
      "Epoch: [16][600/782]\tTime 0.040 (0.044)\tData 0.001 (0.003)\tLoss 0.2883 (0.4672)\tPrec 92.188% (84.336%)\n",
      "Epoch: [16][700/782]\tTime 0.053 (0.044)\tData 0.005 (0.003)\tLoss 0.3094 (0.4693)\tPrec 90.625% (84.194%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.123 (0.123)\tLoss 0.5224 (0.5224)\tPrec 82.812% (82.812%)\n",
      "Test: [100/157]\tTime 0.020 (0.023)\tLoss 0.7262 (0.5307)\tPrec 79.688% (83.230%)\n",
      " * Prec 82.810% \n",
      "best acc: 82.810000\n",
      "Epoch: [17][0/782]\tTime 0.184 (0.184)\tData 0.102 (0.102)\tLoss 0.4764 (0.4764)\tPrec 81.250% (81.250%)\n",
      "Epoch: [17][100/782]\tTime 0.036 (0.043)\tData 0.002 (0.003)\tLoss 0.3925 (0.4263)\tPrec 89.062% (85.876%)\n",
      "Epoch: [17][200/782]\tTime 0.039 (0.047)\tData 0.002 (0.004)\tLoss 0.5115 (0.4276)\tPrec 84.375% (85.774%)\n",
      "Epoch: [17][300/782]\tTime 0.038 (0.045)\tData 0.002 (0.003)\tLoss 0.2836 (0.4333)\tPrec 89.062% (85.382%)\n",
      "Epoch: [17][400/782]\tTime 0.040 (0.044)\tData 0.002 (0.003)\tLoss 0.4939 (0.4400)\tPrec 82.812% (85.147%)\n",
      "Epoch: [17][500/782]\tTime 0.040 (0.045)\tData 0.001 (0.003)\tLoss 0.4694 (0.4391)\tPrec 85.938% (85.173%)\n",
      "Epoch: [17][600/782]\tTime 0.040 (0.045)\tData 0.002 (0.003)\tLoss 0.2568 (0.4393)\tPrec 92.188% (85.186%)\n",
      "Epoch: [17][700/782]\tTime 0.040 (0.044)\tData 0.001 (0.003)\tLoss 0.3717 (0.4407)\tPrec 82.812% (85.137%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.131 (0.131)\tLoss 0.7652 (0.7652)\tPrec 78.125% (78.125%)\n",
      "Test: [100/157]\tTime 0.023 (0.023)\tLoss 0.5748 (0.5882)\tPrec 84.375% (80.941%)\n",
      " * Prec 80.720% \n",
      "best acc: 82.810000\n",
      "Epoch: [18][0/782]\tTime 0.192 (0.192)\tData 0.123 (0.123)\tLoss 0.4174 (0.4174)\tPrec 81.250% (81.250%)\n",
      "Epoch: [18][100/782]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.3243 (0.4043)\tPrec 84.375% (86.355%)\n",
      "Epoch: [18][200/782]\tTime 0.065 (0.046)\tData 0.009 (0.004)\tLoss 0.5033 (0.4153)\tPrec 76.562% (86.054%)\n",
      "Epoch: [18][300/782]\tTime 0.043 (0.045)\tData 0.008 (0.004)\tLoss 0.3991 (0.4163)\tPrec 90.625% (85.963%)\n",
      "Epoch: [18][400/782]\tTime 0.038 (0.044)\tData 0.002 (0.003)\tLoss 0.3825 (0.4220)\tPrec 84.375% (85.731%)\n",
      "Epoch: [18][500/782]\tTime 0.050 (0.045)\tData 0.007 (0.003)\tLoss 0.2255 (0.4258)\tPrec 92.188% (85.666%)\n",
      "Epoch: [18][600/782]\tTime 0.039 (0.044)\tData 0.001 (0.003)\tLoss 0.4334 (0.4285)\tPrec 85.938% (85.574%)\n",
      "Epoch: [18][700/782]\tTime 0.040 (0.044)\tData 0.002 (0.003)\tLoss 0.5291 (0.4300)\tPrec 79.688% (85.496%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.197 (0.197)\tLoss 0.5353 (0.5353)\tPrec 79.688% (79.688%)\n",
      "Test: [100/157]\tTime 0.022 (0.029)\tLoss 0.4629 (0.4935)\tPrec 82.812% (83.880%)\n",
      " * Prec 83.800% \n",
      "best acc: 83.800000\n",
      "Epoch: [19][0/782]\tTime 0.185 (0.185)\tData 0.121 (0.121)\tLoss 0.4249 (0.4249)\tPrec 85.938% (85.938%)\n",
      "Epoch: [19][100/782]\tTime 0.039 (0.043)\tData 0.001 (0.003)\tLoss 0.4487 (0.4263)\tPrec 87.500% (85.767%)\n",
      "Epoch: [19][200/782]\tTime 0.049 (0.044)\tData 0.001 (0.003)\tLoss 0.3364 (0.4000)\tPrec 85.938% (86.676%)\n",
      "Epoch: [19][300/782]\tTime 0.039 (0.045)\tData 0.002 (0.003)\tLoss 0.4220 (0.4055)\tPrec 89.062% (86.389%)\n",
      "Epoch: [19][400/782]\tTime 0.040 (0.044)\tData 0.001 (0.003)\tLoss 0.2393 (0.4129)\tPrec 89.062% (86.093%)\n",
      "Epoch: [19][500/782]\tTime 0.052 (0.044)\tData 0.006 (0.003)\tLoss 0.5498 (0.4143)\tPrec 76.562% (85.959%)\n",
      "Epoch: [19][600/782]\tTime 0.059 (0.044)\tData 0.002 (0.003)\tLoss 0.5513 (0.4151)\tPrec 79.688% (85.886%)\n",
      "Epoch: [19][700/782]\tTime 0.038 (0.044)\tData 0.001 (0.003)\tLoss 0.3305 (0.4153)\tPrec 89.062% (85.859%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.137 (0.137)\tLoss 0.5081 (0.5081)\tPrec 84.375% (84.375%)\n",
      "Test: [100/157]\tTime 0.021 (0.034)\tLoss 0.6518 (0.4858)\tPrec 84.375% (84.499%)\n",
      " * Prec 84.640% \n",
      "best acc: 84.640000\n",
      "Epoch: [20][0/782]\tTime 0.191 (0.191)\tData 0.130 (0.130)\tLoss 0.4018 (0.4018)\tPrec 87.500% (87.500%)\n",
      "Epoch: [20][100/782]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.3915 (0.3937)\tPrec 85.938% (86.742%)\n",
      "Epoch: [20][200/782]\tTime 0.058 (0.041)\tData 0.006 (0.003)\tLoss 0.2635 (0.3957)\tPrec 92.188% (86.451%)\n",
      "Epoch: [20][300/782]\tTime 0.039 (0.044)\tData 0.002 (0.003)\tLoss 0.4553 (0.3954)\tPrec 87.500% (86.649%)\n",
      "Epoch: [20][400/782]\tTime 0.039 (0.043)\tData 0.001 (0.003)\tLoss 0.3102 (0.3923)\tPrec 87.500% (86.764%)\n",
      "Epoch: [20][500/782]\tTime 0.038 (0.043)\tData 0.001 (0.003)\tLoss 0.4767 (0.3926)\tPrec 82.812% (86.748%)\n",
      "Epoch: [20][600/782]\tTime 0.045 (0.044)\tData 0.003 (0.003)\tLoss 0.2830 (0.3940)\tPrec 90.625% (86.715%)\n",
      "Epoch: [20][700/782]\tTime 0.037 (0.044)\tData 0.001 (0.003)\tLoss 0.3558 (0.3982)\tPrec 82.812% (86.533%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.135 (0.135)\tLoss 0.4360 (0.4360)\tPrec 84.375% (84.375%)\n",
      "Test: [100/157]\tTime 0.046 (0.027)\tLoss 0.5211 (0.4703)\tPrec 81.250% (84.684%)\n",
      " * Prec 84.830% \n",
      "best acc: 84.830000\n",
      "Epoch: [21][0/782]\tTime 0.190 (0.190)\tData 0.135 (0.135)\tLoss 0.4131 (0.4131)\tPrec 90.625% (90.625%)\n",
      "Epoch: [21][100/782]\tTime 0.041 (0.042)\tData 0.001 (0.003)\tLoss 0.2866 (0.3713)\tPrec 89.062% (87.423%)\n",
      "Epoch: [21][200/782]\tTime 0.040 (0.041)\tData 0.002 (0.003)\tLoss 0.4541 (0.3782)\tPrec 82.812% (87.243%)\n",
      "Epoch: [21][300/782]\tTime 0.039 (0.045)\tData 0.002 (0.003)\tLoss 0.2356 (0.3814)\tPrec 95.312% (87.126%)\n",
      "Epoch: [21][400/782]\tTime 0.042 (0.044)\tData 0.002 (0.003)\tLoss 0.4641 (0.3762)\tPrec 84.375% (87.344%)\n",
      "Epoch: [21][500/782]\tTime 0.040 (0.043)\tData 0.002 (0.003)\tLoss 0.2657 (0.3772)\tPrec 92.188% (87.341%)\n",
      "Epoch: [21][600/782]\tTime 0.039 (0.045)\tData 0.002 (0.003)\tLoss 0.3912 (0.3783)\tPrec 84.375% (87.253%)\n",
      "Epoch: [21][700/782]\tTime 0.040 (0.044)\tData 0.002 (0.003)\tLoss 0.4030 (0.3795)\tPrec 89.062% (87.235%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.118 (0.118)\tLoss 0.4097 (0.4097)\tPrec 84.375% (84.375%)\n",
      "Test: [100/157]\tTime 0.025 (0.023)\tLoss 0.8031 (0.4952)\tPrec 81.250% (83.571%)\n",
      " * Prec 83.810% \n",
      "best acc: 84.830000\n",
      "Epoch: [22][0/782]\tTime 0.293 (0.293)\tData 0.226 (0.226)\tLoss 0.5348 (0.5348)\tPrec 84.375% (84.375%)\n",
      "Epoch: [22][100/782]\tTime 0.039 (0.047)\tData 0.002 (0.005)\tLoss 0.2131 (0.3478)\tPrec 93.750% (88.475%)\n",
      "Epoch: [22][200/782]\tTime 0.044 (0.044)\tData 0.002 (0.004)\tLoss 0.4649 (0.3589)\tPrec 81.250% (88.036%)\n",
      "Epoch: [22][300/782]\tTime 0.065 (0.045)\tData 0.005 (0.004)\tLoss 0.2687 (0.3656)\tPrec 90.625% (87.583%)\n",
      "Epoch: [22][400/782]\tTime 0.039 (0.045)\tData 0.002 (0.004)\tLoss 0.2493 (0.3629)\tPrec 92.188% (87.765%)\n",
      "Epoch: [22][500/782]\tTime 0.039 (0.044)\tData 0.001 (0.003)\tLoss 0.5440 (0.3626)\tPrec 82.812% (87.771%)\n",
      "Epoch: [22][600/782]\tTime 0.057 (0.045)\tData 0.007 (0.003)\tLoss 0.3378 (0.3659)\tPrec 89.062% (87.739%)\n",
      "Epoch: [22][700/782]\tTime 0.040 (0.045)\tData 0.004 (0.003)\tLoss 0.2420 (0.3666)\tPrec 89.062% (87.721%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.131 (0.131)\tLoss 0.4455 (0.4455)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.044 (0.024)\tLoss 0.4590 (0.4877)\tPrec 85.938% (84.452%)\n",
      " * Prec 84.600% \n",
      "best acc: 84.830000\n",
      "Epoch: [23][0/782]\tTime 0.256 (0.256)\tData 0.164 (0.164)\tLoss 0.2839 (0.2839)\tPrec 92.188% (92.188%)\n",
      "Epoch: [23][100/782]\tTime 0.040 (0.052)\tData 0.004 (0.005)\tLoss 0.3964 (0.3459)\tPrec 85.938% (88.351%)\n",
      "Epoch: [23][200/782]\tTime 0.041 (0.046)\tData 0.008 (0.004)\tLoss 0.2896 (0.3455)\tPrec 90.625% (88.347%)\n",
      "Epoch: [23][300/782]\tTime 0.051 (0.045)\tData 0.002 (0.004)\tLoss 0.6544 (0.3504)\tPrec 78.125% (88.128%)\n",
      "Epoch: [23][400/782]\tTime 0.039 (0.046)\tData 0.001 (0.004)\tLoss 0.4564 (0.3560)\tPrec 84.375% (88.049%)\n",
      "Epoch: [23][500/782]\tTime 0.040 (0.045)\tData 0.002 (0.003)\tLoss 0.1789 (0.3542)\tPrec 93.750% (88.146%)\n",
      "Epoch: [23][600/782]\tTime 0.063 (0.045)\tData 0.002 (0.003)\tLoss 0.2293 (0.3541)\tPrec 92.188% (88.163%)\n",
      "Epoch: [23][700/782]\tTime 0.046 (0.046)\tData 0.002 (0.003)\tLoss 0.3115 (0.3566)\tPrec 89.062% (88.091%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.119 (0.119)\tLoss 0.2923 (0.2923)\tPrec 90.625% (90.625%)\n",
      "Test: [100/157]\tTime 0.020 (0.023)\tLoss 0.4777 (0.4271)\tPrec 84.375% (85.953%)\n",
      " * Prec 85.700% \n",
      "best acc: 85.700000\n",
      "Epoch: [24][0/782]\tTime 0.165 (0.165)\tData 0.107 (0.107)\tLoss 0.2151 (0.2151)\tPrec 90.625% (90.625%)\n",
      "Epoch: [24][100/782]\tTime 0.058 (0.066)\tData 0.002 (0.007)\tLoss 0.4792 (0.3082)\tPrec 85.938% (89.913%)\n",
      "Epoch: [24][200/782]\tTime 0.040 (0.059)\tData 0.003 (0.006)\tLoss 0.3052 (0.3221)\tPrec 89.062% (89.280%)\n",
      "Epoch: [24][300/782]\tTime 0.037 (0.053)\tData 0.001 (0.005)\tLoss 0.2054 (0.3279)\tPrec 95.312% (89.203%)\n",
      "Epoch: [24][400/782]\tTime 0.057 (0.051)\tData 0.002 (0.004)\tLoss 0.5549 (0.3317)\tPrec 78.125% (89.062%)\n",
      "Epoch: [24][500/782]\tTime 0.038 (0.050)\tData 0.006 (0.004)\tLoss 0.7164 (0.3338)\tPrec 82.812% (89.031%)\n",
      "Epoch: [24][600/782]\tTime 0.060 (0.049)\tData 0.014 (0.004)\tLoss 0.5570 (0.3372)\tPrec 82.812% (88.943%)\n",
      "Epoch: [24][700/782]\tTime 0.069 (0.048)\tData 0.002 (0.004)\tLoss 0.6322 (0.3392)\tPrec 82.812% (88.817%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.128 (0.128)\tLoss 0.5370 (0.5370)\tPrec 84.375% (84.375%)\n",
      "Test: [100/157]\tTime 0.023 (0.023)\tLoss 0.5654 (0.5066)\tPrec 81.250% (84.390%)\n",
      " * Prec 84.020% \n",
      "best acc: 85.700000\n",
      "Epoch: [25][0/782]\tTime 0.182 (0.182)\tData 0.112 (0.112)\tLoss 0.4369 (0.4369)\tPrec 87.500% (87.500%)\n",
      "Epoch: [25][100/782]\tTime 0.055 (0.043)\tData 0.002 (0.004)\tLoss 0.2143 (0.3300)\tPrec 92.188% (89.109%)\n",
      "Epoch: [25][200/782]\tTime 0.042 (0.047)\tData 0.002 (0.004)\tLoss 0.3013 (0.3270)\tPrec 90.625% (89.078%)\n",
      "Epoch: [25][300/782]\tTime 0.040 (0.045)\tData 0.002 (0.003)\tLoss 0.3088 (0.3276)\tPrec 92.188% (88.839%)\n",
      "Epoch: [25][400/782]\tTime 0.059 (0.044)\tData 0.004 (0.003)\tLoss 0.2065 (0.3242)\tPrec 93.750% (89.175%)\n",
      "Epoch: [25][500/782]\tTime 0.039 (0.046)\tData 0.001 (0.004)\tLoss 0.3642 (0.3276)\tPrec 89.062% (89.084%)\n",
      "Epoch: [25][600/782]\tTime 0.039 (0.045)\tData 0.002 (0.003)\tLoss 0.2140 (0.3307)\tPrec 93.750% (88.912%)\n",
      "Epoch: [25][700/782]\tTime 0.039 (0.044)\tData 0.002 (0.003)\tLoss 0.4531 (0.3335)\tPrec 84.375% (88.884%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.142 (0.142)\tLoss 0.3526 (0.3526)\tPrec 87.500% (87.500%)\n",
      "Test: [100/157]\tTime 0.020 (0.023)\tLoss 0.5506 (0.4589)\tPrec 84.375% (85.257%)\n",
      " * Prec 85.120% \n",
      "best acc: 85.700000\n",
      "Epoch: [26][0/782]\tTime 0.181 (0.181)\tData 0.111 (0.111)\tLoss 0.3057 (0.3057)\tPrec 87.500% (87.500%)\n",
      "Epoch: [26][100/782]\tTime 0.040 (0.043)\tData 0.002 (0.003)\tLoss 0.2758 (0.3162)\tPrec 85.938% (89.542%)\n",
      "Epoch: [26][200/782]\tTime 0.040 (0.047)\tData 0.003 (0.004)\tLoss 0.2252 (0.3124)\tPrec 90.625% (89.428%)\n",
      "Epoch: [26][300/782]\tTime 0.040 (0.045)\tData 0.001 (0.003)\tLoss 0.2837 (0.3179)\tPrec 85.938% (89.239%)\n",
      "Epoch: [26][400/782]\tTime 0.039 (0.044)\tData 0.002 (0.003)\tLoss 0.4291 (0.3197)\tPrec 87.500% (89.195%)\n",
      "Epoch: [26][500/782]\tTime 0.076 (0.045)\tData 0.005 (0.003)\tLoss 0.3908 (0.3217)\tPrec 87.500% (89.091%)\n",
      "Epoch: [26][600/782]\tTime 0.040 (0.045)\tData 0.003 (0.003)\tLoss 0.1811 (0.3252)\tPrec 92.188% (89.003%)\n",
      "Epoch: [26][700/782]\tTime 0.040 (0.044)\tData 0.002 (0.003)\tLoss 0.2517 (0.3290)\tPrec 89.062% (88.900%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.153 (0.153)\tLoss 0.7318 (0.7318)\tPrec 79.688% (79.688%)\n",
      "Test: [100/157]\tTime 0.019 (0.028)\tLoss 0.6669 (0.4545)\tPrec 82.812% (86.061%)\n",
      " * Prec 86.110% \n",
      "best acc: 86.110000\n",
      "Epoch: [27][0/782]\tTime 0.174 (0.174)\tData 0.106 (0.106)\tLoss 0.5275 (0.5275)\tPrec 81.250% (81.250%)\n",
      "Epoch: [27][100/782]\tTime 0.041 (0.043)\tData 0.002 (0.004)\tLoss 0.1649 (0.3344)\tPrec 96.875% (88.892%)\n",
      "Epoch: [27][200/782]\tTime 0.048 (0.044)\tData 0.004 (0.004)\tLoss 0.4450 (0.3239)\tPrec 81.250% (89.195%)\n",
      "Epoch: [27][300/782]\tTime 0.040 (0.045)\tData 0.002 (0.003)\tLoss 0.3314 (0.3171)\tPrec 93.750% (89.410%)\n",
      "Epoch: [27][400/782]\tTime 0.039 (0.044)\tData 0.002 (0.003)\tLoss 0.2378 (0.3204)\tPrec 92.188% (89.292%)\n",
      "Epoch: [27][500/782]\tTime 0.061 (0.044)\tData 0.002 (0.003)\tLoss 0.2631 (0.3174)\tPrec 92.188% (89.490%)\n",
      "Epoch: [27][600/782]\tTime 0.044 (0.045)\tData 0.008 (0.003)\tLoss 0.2728 (0.3171)\tPrec 89.062% (89.450%)\n",
      "Epoch: [27][700/782]\tTime 0.038 (0.044)\tData 0.002 (0.003)\tLoss 0.2509 (0.3160)\tPrec 93.750% (89.457%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.167 (0.167)\tLoss 0.6019 (0.6019)\tPrec 82.812% (82.812%)\n",
      "Test: [100/157]\tTime 0.025 (0.033)\tLoss 0.6529 (0.5327)\tPrec 79.688% (83.601%)\n",
      " * Prec 83.780% \n",
      "best acc: 86.110000\n",
      "Epoch: [28][0/782]\tTime 0.190 (0.190)\tData 0.115 (0.115)\tLoss 0.1591 (0.1591)\tPrec 93.750% (93.750%)\n",
      "Epoch: [28][100/782]\tTime 0.039 (0.042)\tData 0.001 (0.004)\tLoss 0.4621 (0.3409)\tPrec 85.938% (88.506%)\n",
      "Epoch: [28][200/782]\tTime 0.038 (0.041)\tData 0.001 (0.003)\tLoss 0.3645 (0.3157)\tPrec 90.625% (89.599%)\n",
      "Epoch: [28][300/782]\tTime 0.041 (0.045)\tData 0.001 (0.004)\tLoss 0.1907 (0.3113)\tPrec 93.750% (89.644%)\n",
      "Epoch: [28][400/782]\tTime 0.042 (0.044)\tData 0.005 (0.003)\tLoss 0.3614 (0.3083)\tPrec 87.500% (89.787%)\n",
      "Epoch: [28][500/782]\tTime 0.037 (0.043)\tData 0.002 (0.003)\tLoss 0.4247 (0.3082)\tPrec 82.812% (89.777%)\n",
      "Epoch: [28][600/782]\tTime 0.042 (0.044)\tData 0.003 (0.004)\tLoss 0.2903 (0.3076)\tPrec 90.625% (89.790%)\n",
      "Epoch: [28][700/782]\tTime 0.044 (0.044)\tData 0.002 (0.003)\tLoss 0.3774 (0.3067)\tPrec 84.375% (89.758%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.125 (0.125)\tLoss 0.4529 (0.4529)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.045 (0.028)\tLoss 0.6208 (0.4760)\tPrec 82.812% (84.112%)\n",
      " * Prec 84.480% \n",
      "best acc: 86.110000\n",
      "Epoch: [29][0/782]\tTime 0.169 (0.169)\tData 0.108 (0.108)\tLoss 0.2293 (0.2293)\tPrec 90.625% (90.625%)\n",
      "Epoch: [29][100/782]\tTime 0.040 (0.043)\tData 0.002 (0.004)\tLoss 0.2077 (0.2828)\tPrec 92.188% (90.610%)\n",
      "Epoch: [29][200/782]\tTime 0.040 (0.042)\tData 0.004 (0.003)\tLoss 0.4052 (0.2897)\tPrec 87.500% (90.330%)\n",
      "Epoch: [29][300/782]\tTime 0.046 (0.045)\tData 0.002 (0.004)\tLoss 0.2969 (0.2894)\tPrec 92.188% (90.282%)\n",
      "Epoch: [29][400/782]\tTime 0.040 (0.044)\tData 0.002 (0.004)\tLoss 0.5113 (0.2935)\tPrec 85.938% (90.161%)\n",
      "Epoch: [29][500/782]\tTime 0.039 (0.043)\tData 0.002 (0.003)\tLoss 0.3035 (0.2925)\tPrec 87.500% (90.173%)\n",
      "Epoch: [29][600/782]\tTime 0.060 (0.045)\tData 0.002 (0.004)\tLoss 0.1619 (0.2930)\tPrec 96.875% (90.191%)\n",
      "Epoch: [29][700/782]\tTime 0.041 (0.044)\tData 0.002 (0.003)\tLoss 0.4682 (0.2951)\tPrec 87.500% (90.137%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.136 (0.136)\tLoss 0.5356 (0.5356)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.024 (0.024)\tLoss 0.4709 (0.4159)\tPrec 87.500% (86.556%)\n",
      " * Prec 86.680% \n",
      "best acc: 86.680000\n",
      "Epoch: [30][0/782]\tTime 0.274 (0.274)\tData 0.192 (0.192)\tLoss 0.4199 (0.4199)\tPrec 84.375% (84.375%)\n",
      "Epoch: [30][100/782]\tTime 0.039 (0.048)\tData 0.004 (0.005)\tLoss 0.1760 (0.2704)\tPrec 93.750% (90.610%)\n",
      "Epoch: [30][200/782]\tTime 0.038 (0.044)\tData 0.003 (0.004)\tLoss 0.3746 (0.2749)\tPrec 90.625% (90.633%)\n",
      "Epoch: [30][300/782]\tTime 0.068 (0.045)\tData 0.026 (0.004)\tLoss 0.3192 (0.2794)\tPrec 87.500% (90.594%)\n",
      "Epoch: [30][400/782]\tTime 0.039 (0.045)\tData 0.004 (0.004)\tLoss 0.1639 (0.2813)\tPrec 90.625% (90.539%)\n",
      "Epoch: [30][500/782]\tTime 0.043 (0.044)\tData 0.002 (0.004)\tLoss 0.2117 (0.2792)\tPrec 90.625% (90.581%)\n",
      "Epoch: [30][600/782]\tTime 0.053 (0.045)\tData 0.006 (0.004)\tLoss 0.3985 (0.2831)\tPrec 89.062% (90.459%)\n",
      "Epoch: [30][700/782]\tTime 0.040 (0.045)\tData 0.003 (0.004)\tLoss 0.4826 (0.2847)\tPrec 81.250% (90.395%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.129 (0.129)\tLoss 0.3557 (0.3557)\tPrec 87.500% (87.500%)\n",
      "Test: [100/157]\tTime 0.015 (0.023)\tLoss 0.6316 (0.4546)\tPrec 82.812% (86.015%)\n",
      " * Prec 85.880% \n",
      "best acc: 86.680000\n",
      "Epoch: [31][0/782]\tTime 0.254 (0.254)\tData 0.137 (0.137)\tLoss 0.3340 (0.3340)\tPrec 92.188% (92.188%)\n",
      "Epoch: [31][100/782]\tTime 0.040 (0.052)\tData 0.005 (0.006)\tLoss 0.2716 (0.2730)\tPrec 92.188% (91.197%)\n",
      "Epoch: [31][200/782]\tTime 0.039 (0.047)\tData 0.005 (0.004)\tLoss 0.1500 (0.2726)\tPrec 96.875% (90.819%)\n",
      "Epoch: [31][300/782]\tTime 0.055 (0.046)\tData 0.016 (0.004)\tLoss 0.1673 (0.2766)\tPrec 93.750% (90.667%)\n",
      "Epoch: [31][400/782]\tTime 0.039 (0.047)\tData 0.003 (0.004)\tLoss 0.2605 (0.2793)\tPrec 92.188% (90.687%)\n",
      "Epoch: [31][500/782]\tTime 0.042 (0.046)\tData 0.001 (0.004)\tLoss 0.2917 (0.2813)\tPrec 92.188% (90.659%)\n",
      "Epoch: [31][600/782]\tTime 0.070 (0.045)\tData 0.000 (0.003)\tLoss 0.2644 (0.2820)\tPrec 90.625% (90.677%)\n",
      "Epoch: [31][700/782]\tTime 0.039 (0.046)\tData 0.001 (0.004)\tLoss 0.4150 (0.2832)\tPrec 84.375% (90.627%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.144 (0.144)\tLoss 0.3699 (0.3699)\tPrec 87.500% (87.500%)\n",
      "Test: [100/157]\tTime 0.017 (0.023)\tLoss 0.7146 (0.4447)\tPrec 79.688% (85.613%)\n",
      " * Prec 85.500% \n",
      "best acc: 86.680000\n",
      "Epoch: [32][0/782]\tTime 0.194 (0.194)\tData 0.116 (0.116)\tLoss 0.2688 (0.2688)\tPrec 90.625% (90.625%)\n",
      "Epoch: [32][100/782]\tTime 0.040 (0.054)\tData 0.002 (0.005)\tLoss 0.2556 (0.2367)\tPrec 85.938% (92.048%)\n",
      "Epoch: [32][200/782]\tTime 0.040 (0.048)\tData 0.002 (0.004)\tLoss 0.4262 (0.2623)\tPrec 90.625% (91.255%)\n",
      "Epoch: [32][300/782]\tTime 0.044 (0.046)\tData 0.002 (0.004)\tLoss 0.2807 (0.2689)\tPrec 89.062% (91.035%)\n",
      "Epoch: [32][400/782]\tTime 0.044 (0.047)\tData 0.009 (0.004)\tLoss 0.2273 (0.2698)\tPrec 95.312% (90.972%)\n",
      "Epoch: [32][500/782]\tTime 0.040 (0.046)\tData 0.002 (0.004)\tLoss 0.2106 (0.2721)\tPrec 93.750% (90.887%)\n",
      "Epoch: [32][600/782]\tTime 0.039 (0.045)\tData 0.002 (0.004)\tLoss 0.3020 (0.2737)\tPrec 87.500% (90.820%)\n",
      "Epoch: [32][700/782]\tTime 0.040 (0.046)\tData 0.001 (0.004)\tLoss 0.2976 (0.2733)\tPrec 90.625% (90.837%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.129 (0.129)\tLoss 0.5025 (0.5025)\tPrec 82.812% (82.812%)\n",
      "Test: [100/157]\tTime 0.025 (0.024)\tLoss 0.4302 (0.4971)\tPrec 89.062% (84.932%)\n",
      " * Prec 84.800% \n",
      "best acc: 86.680000\n",
      "Epoch: [33][0/782]\tTime 0.174 (0.174)\tData 0.121 (0.121)\tLoss 0.1743 (0.1743)\tPrec 92.188% (92.188%)\n",
      "Epoch: [33][100/782]\tTime 0.039 (0.055)\tData 0.002 (0.007)\tLoss 0.1568 (0.2613)\tPrec 93.750% (91.228%)\n",
      "Epoch: [33][200/782]\tTime 0.038 (0.049)\tData 0.001 (0.005)\tLoss 0.2525 (0.2622)\tPrec 92.188% (91.309%)\n",
      "Epoch: [33][300/782]\tTime 0.048 (0.046)\tData 0.005 (0.004)\tLoss 0.1256 (0.2662)\tPrec 93.750% (91.123%)\n",
      "Epoch: [33][400/782]\tTime 0.053 (0.048)\tData 0.004 (0.004)\tLoss 0.4170 (0.2664)\tPrec 85.938% (91.073%)\n",
      "Epoch: [33][500/782]\tTime 0.046 (0.047)\tData 0.003 (0.004)\tLoss 0.2406 (0.2724)\tPrec 90.625% (90.875%)\n",
      "Epoch: [33][600/782]\tTime 0.038 (0.046)\tData 0.002 (0.004)\tLoss 0.3220 (0.2710)\tPrec 85.938% (90.927%)\n",
      "Epoch: [33][700/782]\tTime 0.040 (0.047)\tData 0.002 (0.004)\tLoss 0.2397 (0.2731)\tPrec 90.625% (90.857%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.142 (0.142)\tLoss 0.4641 (0.4641)\tPrec 87.500% (87.500%)\n",
      "Test: [100/157]\tTime 0.028 (0.024)\tLoss 0.3725 (0.4406)\tPrec 87.500% (86.386%)\n",
      " * Prec 86.450% \n",
      "best acc: 86.680000\n",
      "Epoch: [34][0/782]\tTime 0.171 (0.171)\tData 0.110 (0.110)\tLoss 0.2369 (0.2369)\tPrec 89.062% (89.062%)\n",
      "Epoch: [34][100/782]\tTime 0.059 (0.052)\tData 0.002 (0.006)\tLoss 0.1625 (0.2885)\tPrec 92.188% (90.594%)\n",
      "Epoch: [34][200/782]\tTime 0.041 (0.048)\tData 0.002 (0.005)\tLoss 0.2451 (0.2686)\tPrec 89.062% (91.255%)\n",
      "Epoch: [34][300/782]\tTime 0.040 (0.046)\tData 0.002 (0.004)\tLoss 0.2868 (0.2664)\tPrec 87.500% (91.321%)\n",
      "Epoch: [34][400/782]\tTime 0.057 (0.047)\tData 0.000 (0.005)\tLoss 0.2134 (0.2659)\tPrec 93.750% (91.217%)\n",
      "Epoch: [34][500/782]\tTime 0.043 (0.046)\tData 0.002 (0.004)\tLoss 0.3261 (0.2687)\tPrec 85.938% (91.233%)\n",
      "Epoch: [34][600/782]\tTime 0.041 (0.046)\tData 0.002 (0.004)\tLoss 0.1965 (0.2683)\tPrec 93.750% (91.252%)\n",
      "Epoch: [34][700/782]\tTime 0.066 (0.048)\tData 0.004 (0.005)\tLoss 0.2490 (0.2655)\tPrec 89.062% (91.289%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.136 (0.136)\tLoss 0.3575 (0.3575)\tPrec 87.500% (87.500%)\n",
      "Test: [100/157]\tTime 0.028 (0.027)\tLoss 0.4868 (0.4635)\tPrec 85.938% (85.381%)\n",
      " * Prec 85.570% \n",
      "best acc: 86.680000\n",
      "Epoch: [35][0/782]\tTime 0.181 (0.181)\tData 0.113 (0.113)\tLoss 0.3570 (0.3570)\tPrec 85.938% (85.938%)\n",
      "Epoch: [35][100/782]\tTime 0.044 (0.049)\tData 0.004 (0.005)\tLoss 0.0919 (0.1977)\tPrec 96.875% (93.162%)\n",
      "Epoch: [35][200/782]\tTime 0.038 (0.048)\tData 0.002 (0.004)\tLoss 0.1579 (0.1844)\tPrec 95.312% (93.804%)\n",
      "Epoch: [35][300/782]\tTime 0.040 (0.046)\tData 0.002 (0.004)\tLoss 0.3605 (0.1739)\tPrec 87.500% (94.139%)\n",
      "Epoch: [35][400/782]\tTime 0.059 (0.046)\tData 0.007 (0.004)\tLoss 0.1290 (0.1688)\tPrec 95.312% (94.334%)\n",
      "Epoch: [35][500/782]\tTime 0.040 (0.046)\tData 0.003 (0.004)\tLoss 0.1278 (0.1639)\tPrec 93.750% (94.483%)\n",
      "Epoch: [35][600/782]\tTime 0.040 (0.045)\tData 0.004 (0.004)\tLoss 0.0551 (0.1614)\tPrec 96.875% (94.525%)\n",
      "Epoch: [35][700/782]\tTime 0.068 (0.046)\tData 0.004 (0.004)\tLoss 0.1900 (0.1605)\tPrec 90.625% (94.593%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.137 (0.137)\tLoss 0.3197 (0.3197)\tPrec 90.625% (90.625%)\n",
      "Test: [100/157]\tTime 0.022 (0.024)\tLoss 0.3625 (0.3247)\tPrec 85.938% (89.929%)\n",
      " * Prec 89.900% \n",
      "best acc: 89.900000\n",
      "Epoch: [36][0/782]\tTime 0.200 (0.200)\tData 0.126 (0.126)\tLoss 0.1759 (0.1759)\tPrec 93.750% (93.750%)\n",
      "Epoch: [36][100/782]\tTime 0.061 (0.047)\tData 0.002 (0.005)\tLoss 0.1390 (0.1312)\tPrec 93.750% (95.545%)\n",
      "Epoch: [36][200/782]\tTime 0.040 (0.048)\tData 0.002 (0.004)\tLoss 0.3990 (0.1281)\tPrec 89.062% (95.678%)\n",
      "Epoch: [36][300/782]\tTime 0.044 (0.046)\tData 0.007 (0.004)\tLoss 0.1337 (0.1270)\tPrec 95.312% (95.764%)\n",
      "Epoch: [36][400/782]\tTime 0.073 (0.045)\tData 0.002 (0.004)\tLoss 0.1160 (0.1289)\tPrec 96.875% (95.737%)\n",
      "Epoch: [36][500/782]\tTime 0.040 (0.046)\tData 0.002 (0.004)\tLoss 0.0860 (0.1305)\tPrec 96.875% (95.599%)\n",
      "Epoch: [36][600/782]\tTime 0.039 (0.046)\tData 0.004 (0.004)\tLoss 0.1245 (0.1291)\tPrec 96.875% (95.630%)\n",
      "Epoch: [36][700/782]\tTime 0.065 (0.045)\tData 0.018 (0.004)\tLoss 0.0863 (0.1282)\tPrec 96.875% (95.654%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.139 (0.139)\tLoss 0.2734 (0.2734)\tPrec 90.625% (90.625%)\n",
      "Test: [100/157]\tTime 0.019 (0.024)\tLoss 0.2776 (0.3354)\tPrec 90.625% (89.991%)\n",
      " * Prec 90.140% \n",
      "best acc: 90.140000\n",
      "Epoch: [37][0/782]\tTime 0.185 (0.185)\tData 0.118 (0.118)\tLoss 0.0925 (0.0925)\tPrec 96.875% (96.875%)\n",
      "Epoch: [37][100/782]\tTime 0.046 (0.043)\tData 0.002 (0.004)\tLoss 0.1498 (0.1188)\tPrec 96.875% (95.947%)\n",
      "Epoch: [37][200/782]\tTime 0.039 (0.048)\tData 0.003 (0.004)\tLoss 0.0583 (0.1186)\tPrec 96.875% (95.872%)\n",
      "Epoch: [37][300/782]\tTime 0.039 (0.045)\tData 0.002 (0.004)\tLoss 0.2483 (0.1187)\tPrec 93.750% (96.018%)\n",
      "Epoch: [37][400/782]\tTime 0.039 (0.045)\tData 0.002 (0.004)\tLoss 0.1573 (0.1191)\tPrec 93.750% (96.057%)\n",
      "Epoch: [37][500/782]\tTime 0.044 (0.046)\tData 0.002 (0.004)\tLoss 0.0625 (0.1184)\tPrec 98.438% (96.111%)\n",
      "Epoch: [37][600/782]\tTime 0.046 (0.045)\tData 0.006 (0.004)\tLoss 0.0704 (0.1189)\tPrec 96.875% (96.108%)\n",
      "Epoch: [37][700/782]\tTime 0.040 (0.045)\tData 0.002 (0.004)\tLoss 0.0758 (0.1193)\tPrec 96.875% (96.064%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.132 (0.132)\tLoss 0.2319 (0.2319)\tPrec 92.188% (92.188%)\n",
      "Test: [100/157]\tTime 0.020 (0.024)\tLoss 0.3637 (0.3292)\tPrec 87.500% (90.316%)\n",
      " * Prec 90.380% \n",
      "best acc: 90.380000\n",
      "Epoch: [38][0/782]\tTime 0.197 (0.197)\tData 0.123 (0.123)\tLoss 0.1128 (0.1128)\tPrec 96.875% (96.875%)\n",
      "Epoch: [38][100/782]\tTime 0.043 (0.044)\tData 0.005 (0.004)\tLoss 0.1004 (0.1117)\tPrec 96.875% (96.194%)\n",
      "Epoch: [38][200/782]\tTime 0.040 (0.048)\tData 0.002 (0.004)\tLoss 0.0676 (0.1076)\tPrec 98.438% (96.284%)\n",
      "Epoch: [38][300/782]\tTime 0.039 (0.046)\tData 0.005 (0.004)\tLoss 0.1653 (0.1091)\tPrec 95.312% (96.257%)\n",
      "Epoch: [38][400/782]\tTime 0.039 (0.045)\tData 0.002 (0.004)\tLoss 0.1234 (0.1071)\tPrec 95.312% (96.314%)\n",
      "Epoch: [38][500/782]\tTime 0.039 (0.047)\tData 0.005 (0.004)\tLoss 0.0801 (0.1089)\tPrec 98.438% (96.239%)\n",
      "Epoch: [38][600/782]\tTime 0.039 (0.046)\tData 0.002 (0.004)\tLoss 0.1335 (0.1077)\tPrec 95.312% (96.267%)\n",
      "Epoch: [38][700/782]\tTime 0.045 (0.045)\tData 0.004 (0.004)\tLoss 0.0608 (0.1078)\tPrec 98.438% (96.282%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.145 (0.145)\tLoss 0.2461 (0.2461)\tPrec 93.750% (93.750%)\n",
      "Test: [100/157]\tTime 0.023 (0.024)\tLoss 0.3448 (0.3298)\tPrec 90.625% (90.393%)\n",
      " * Prec 90.490% \n",
      "best acc: 90.490000\n",
      "Epoch: [39][0/782]\tTime 0.171 (0.171)\tData 0.118 (0.118)\tLoss 0.0335 (0.0335)\tPrec 100.000% (100.000%)\n",
      "Epoch: [39][100/782]\tTime 0.040 (0.044)\tData 0.002 (0.004)\tLoss 0.0619 (0.0913)\tPrec 98.438% (96.767%)\n",
      "Epoch: [39][200/782]\tTime 0.039 (0.048)\tData 0.001 (0.004)\tLoss 0.0606 (0.0963)\tPrec 98.438% (96.688%)\n",
      "Epoch: [39][300/782]\tTime 0.041 (0.046)\tData 0.002 (0.004)\tLoss 0.1597 (0.0993)\tPrec 95.312% (96.584%)\n",
      "Epoch: [39][400/782]\tTime 0.041 (0.045)\tData 0.002 (0.004)\tLoss 0.0185 (0.1000)\tPrec 100.000% (96.579%)\n",
      "Epoch: [39][500/782]\tTime 0.043 (0.047)\tData 0.001 (0.004)\tLoss 0.1542 (0.0991)\tPrec 95.312% (96.616%)\n",
      "Epoch: [39][600/782]\tTime 0.040 (0.046)\tData 0.003 (0.004)\tLoss 0.0673 (0.1002)\tPrec 96.875% (96.589%)\n",
      "Epoch: [39][700/782]\tTime 0.040 (0.046)\tData 0.002 (0.004)\tLoss 0.0454 (0.1004)\tPrec 96.875% (96.563%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.128 (0.128)\tLoss 0.2969 (0.2969)\tPrec 92.188% (92.188%)\n",
      "Test: [100/157]\tTime 0.023 (0.024)\tLoss 0.3865 (0.3302)\tPrec 89.062% (90.656%)\n",
      " * Prec 90.650% \n",
      "best acc: 90.650000\n"
     ]
    }
   ],
   "source": [
    "# This cell is from the website\n",
    "\n",
    "lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 40\n",
    "best_prec = 0\n",
    "\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "# weight decay: for regularization to prevent overfitting\n",
    "\n",
    "\n",
    "if not os.path.exists('result_multi'):\n",
    "    os.makedirs('result_multi')\n",
    "\n",
    "fdir = 'result_multi/'+str(model_name)\n",
    "\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entertaining-queensland",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "entertaining-queensland",
    "outputId": "1a1bb0f6-f030-4fbc-8943-cd11f92651df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/157]\tTime 2.333 (2.333)\tLoss 0.2969 (0.2969)\tPrec 92.188% (92.188%)\n",
      "Test: [100/157]\tTime 0.016 (0.045)\tLoss 0.3865 (0.3302)\tPrec 89.062% (90.656%)\n",
      " * Prec 90.650% \n"
     ]
    }
   ],
   "source": [
    "fdir = 'results_multi/'+str(model_name)+'/model_best.pth.tar'\n",
    "\n",
    "checkpoint = torch.load(fdir)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "prec = validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceramic-nigeria",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ceramic-nigeria",
    "outputId": "f297c073-5907-4a78-b69a-43cb40d69f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -th layer prehooked\n",
      "6 -th layer prehooked\n",
      "11 -th layer prehooked\n",
      "15 -th layer prehooked\n",
      "20 -th layer prehooked\n",
      "24 -th layer prehooked\n",
      "28 -th layer prehooked\n",
      "33 -th layer prehooked\n",
      "37 -th layer prehooked\n",
      "40 -th layer prehooked\n",
      "45 -th layer prehooked\n",
      "49 -th layer prehooked\n",
      "53 -th layer prehooked\n",
      "7st convolution's input size: torch.Size([64, 16, 4, 4])\n",
      "7st convolution's input size: torch.Size([64, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "\n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for name, layer in model.named_modules():\n",
    "\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)\n",
    "    i = i+1\n",
    "\n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)\n",
    "\n",
    "print(\"7st convolution's input size:\", save_output.outputs[8][0].size())\n",
    "print(\"7st convolution's input size:\", save_output.outputs[9][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe49613",
   "metadata": {
    "id": "cbe49613"
   },
   "outputs": [],
   "source": [
    "weight_q = model.features[27].weight_q\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha\n",
    "w_bit = 4\n",
    "\n",
    "weight_int = weight_q / (w_alpha / (2**(w_bit-1)-1))\n",
    "#print(weight_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interior-oxygen",
   "metadata": {
    "id": "interior-oxygen"
   },
   "outputs": [],
   "source": [
    "act = save_output.outputs[8][0]\n",
    "act_alpha  = model.features[27].act_alpha\n",
    "act_bit = 4\n",
    "act_quant_fn = act_quantization(act_bit)\n",
    "\n",
    "act_q = act_quant_fn(act, act_alpha)\n",
    "\n",
    "act_int = act_q / (act_alpha / (2**act_bit-1))\n",
    "#print(act_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "victorian-above",
   "metadata": {
    "id": "victorian-above"
   },
   "outputs": [],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 16, out_channels=8, kernel_size = 3, padding=1, bias=False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "psum_int = conv_int(act_int)\n",
    "\n",
    "psum_recovered = psum_int * (act_alpha / (2**act_bit-1)) * (w_alpha / (2**(w_bit-1)-1))\n",
    "relu = model.features[28]\n",
    "psum_recovered = relu(psum_recovered)\n",
    "#print(psum_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "designed-auction",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "designed-auction",
    "outputId": "ad56675f-446c-4320-afab-40249b687395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7829e-07, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs( save_output.outputs[9][0] - psum_recovered )\n",
    "print(difference.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "blind-august",
   "metadata": {
    "id": "blind-august"
   },
   "outputs": [],
   "source": [
    "# act_int.size = torch.Size([128, 64, 32, 32])  <- batch_size, input_ch, ni, nj\n",
    "a_int = act_int[0,:,:,:]  # pick only one input out of batch\n",
    "# a_int.size() = [64, 32, 32]\n",
    "\n",
    "# conv_int.weight.size() = torch.Size([64, 64, 3, 3])  <- output_ch, input_ch, ki, kj\n",
    "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))  # merge ki, kj index to kij\n",
    "# w_int.weight.size() = torch.Size([64, 64, 9])\n",
    "\n",
    "padding = 1\n",
    "stride = 1\n",
    "array_size = 8 # row and column number\n",
    "\n",
    "nig = range(a_int.size(1))  ## ni group\n",
    "njg = range(a_int.size(2))  ## nj group\n",
    "\n",
    "icg = range(int(w_int.size(1)))  ## input channel\n",
    "ocg = range(int(w_int.size(0)))  ## output channel\n",
    "\n",
    "ic_tileg = range(int(len(icg)/array_size))\n",
    "oc_tileg = range(int(len(ocg)/array_size))\n",
    "\n",
    "kijg = range(w_int.size(2))\n",
    "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size\n",
    "\n",
    "######## Padding before Convolution #######\n",
    "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(nig)+padding*2).cuda()\n",
    "# a_pad.size() = [64, 32+2pad, 32+2pad]\n",
    "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
    "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))\n",
    "# a_pad.size() = [64, (32+2pad)*(32+2pad)]\n",
    "\n",
    "\n",
    "a_tile = torch.zeros(len(ic_tileg), array_size,    a_pad.size(1)).cuda()\n",
    "w_tile = torch.zeros(len(oc_tileg)*len(ic_tileg), array_size, array_size, len(kijg)).cuda()\n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    a_tile[ic_tile,:,:] = a_pad[ic_tile*array_size:(ic_tile+1)*array_size,:]\n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    for oc_tile in oc_tileg:\n",
    "        w_tile[oc_tile*len(oc_tileg) + ic_tile,:,:,:] = w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, :]\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "p_nijg = range(a_pad.size(1)) ## psum nij group\n",
    "\n",
    "psum = torch.zeros(len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)).cuda()\n",
    "\n",
    "for kij in kijg:\n",
    "    for ic_tile in ic_tileg:       # Tiling into array_sizeXarray_size array\n",
    "        for oc_tile in oc_tileg:   # Tiling into array_sizeXarray_size array\n",
    "            for nij in p_nijg:       # time domain, sequentially given input\n",
    "                    m = nn.Linear(array_size, array_size, bias=False)\n",
    "                    #m.weight = torch.nn.Parameter(w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, kij])\n",
    "                    m.weight = torch.nn.Parameter(w_tile[len(oc_tileg)*oc_tile+ic_tile,:,:,kij])\n",
    "                    psum[ic_tile, oc_tile, :, nij, kij] = m(a_tile[ic_tile,:,nij]).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subsequent-oracle",
   "metadata": {
    "id": "subsequent-oracle"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "a_pad_ni_dim = int(math.sqrt(a_pad.size(1))) # 32\n",
    "\n",
    "o_ni_dim = int((a_pad_ni_dim - (ki_dim- 1) - 1)/stride + 1)\n",
    "o_nijg = range(o_ni_dim**2)\n",
    "\n",
    "out = torch.zeros(len(ocg), len(o_nijg)).cuda()\n",
    "\n",
    "\n",
    "### SFP accumulation ###\n",
    "for o_nij in o_nijg:\n",
    "    for kij in kijg:\n",
    "        for ic_tile in ic_tileg:\n",
    "            for oc_tile in oc_tileg:\n",
    "                out[oc_tile*array_size:(oc_tile+1)*array_size, o_nij] = out[oc_tile*array_size:(oc_tile+1)*array_size, o_nij] + \\\n",
    "                psum[ic_tile, oc_tile, :, int(o_nij/o_ni_dim)*a_pad_ni_dim + o_nij%o_ni_dim + int(kij/ki_dim)*a_pad_ni_dim + kij%ki_dim, kij]\n",
    "                ## 4th index = (int(o_nij/30)*32 + o_nij%30) + (int(kij/3)*32 + kij%3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "entitled-barbados",
   "metadata": {
    "id": "entitled-barbados"
   },
   "outputs": [],
   "source": [
    "# out_2D = torch.reshape(out, (out.size(0), o_ni_dim, -1)) # nij -> ni & nj\n",
    "# difference = (out_2D - output_int[0,:,:,:])\n",
    "# print(difference.abs().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9972d59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "b9972d59",
    "outputId": "76bb391a-c73e-43f4-eda8-8f5129038c93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.0000, 0.0000,\n",
      "          0.0000, 2.0000, 0.0000, 1.0000, 3.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.0000,\n",
      "          6.0000, 8.0000, 6.0000, 0.0000, 0.0000, 2.0000, 3.0000, 2.0000,\n",
      "          4.0000, 0.0000, 0.0000, 2.0000, 1.0000, 1.0000, 2.0000, 0.0000,\n",
      "          0.0000, 2.0000, 0.0000, 0.0000, 2.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 6.0000, 7.0000, 0.0000, 0.0000, 1.0000, 1.0000, 7.0000,\n",
      "          4.0000, 0.0000, 0.0000, 2.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 5.0000, 3.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 2.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 4.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0000,\n",
      "          4.0000, 5.0000, 3.0000, 0.0000, 0.0000, 3.0000, 2.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 7.0000, 7.0000, 4.0000, 3.0000, 0.0000,\n",
      "          0.0000, 8.0000, 9.0000, 7.0000, 6.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.0000,\n",
      "          6.0000, 8.0000, 6.0000, 0.0000, 0.0000, 7.0000, 7.0000, 8.0000,\n",
      "          5.0000, 0.0000, 0.0000, 6.0000, 7.0000, 6.0000, 4.0000, 0.0000,\n",
      "          0.0000, 3.0000, 3.0000, 3.0000, 2.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0000,\n",
      "          1.0000, 2.0000, 2.0000, 0.0000, 0.0000, 2.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 4.0000, 4.0000, 7.0000, 6.0000, 0.0000,\n",
      "          0.0000, 8.0000, 5.0000, 4.0000, 3.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([2, 8, 36])\n",
      "torch.Size([16, 36])\n",
      "tensor([[0., 0., 0., 0., 1., 4., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 4., 3., 0.]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 4., 0., 0., 0., 0., 2., 0., 2., 4., 3., 0.],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### show this cell partially. The following cells should be printed by students ###\n",
    "tile_id = 0\n",
    "nij = 0 # just a random number\n",
    "X = a_tile[:,:,nij:nij+36]\n",
    "print(X)\n",
    "print(X.size())\n",
    "Y = torch.reshape(X, (-1, X.size(2)))\n",
    "print(Y.size())\n",
    "print(X[:,:,7])\n",
    "#print (X[:,:,0].size())# [tile_num, array row num, time_steps]\n",
    "print(Y[:,7])\n",
    "bit_precision = 4\n",
    "file = open('activation_mc_project.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(Y.size(1)):\n",
    "    for j in range(Y.size(0)): # row #\n",
    "        X_bin = '{0:04b}'.format(round(Y[15-j,i].item()))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(X_bin[k])\n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "minimal-serbia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "minimal-serbia",
    "outputId": "12a5cfb7-6773-4649-eb3f-eefd500bf19a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 36])\n",
      "tensor([[0., 0., 0., 0., 0., 6., 0., 0.],\n",
      "        [0., 0., 0., 0., 4., 6., 1., 0.]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 6., 0., 0., 0., 0., 0., 0., 4., 6., 1., 0.],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### show this cell partially. The following cells should be printed by students ###\n",
    "tile_id = 0\n",
    "nij = 0 # just a random number\n",
    "X = a_tile[:,:,nij:nij+36]\n",
    "Y = torch.reshape(X, (-1, X.size(2)))\n",
    "print(Y.size())\n",
    "print(X[:,:,8])\n",
    "#print (X[:,:,0].size())# [tile_num, array row num, time_steps]\n",
    "print(Y[:,8])\n",
    "bit_precision = 4\n",
    "file = open('activation_mc_project_int.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(Y.size(1)):\n",
    "    for j in range(Y.size(0)): # row #\n",
    "        file.write(f'{round(Y[15-j,i].item())},')\n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f7d413",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "66f7d413",
    "outputId": "f9335f10-2d7c-499d-d201-fabe855d123f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "print(w_tile.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c9e7b31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7c9e7b31",
    "outputId": "0bb5f9cf-1e5d-4167-be0d-de2e3e4a9a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "print(w_tile.size())\n",
    "W = w_tile[:,:,:,0]  # w_tile[tile_num, array col num, array row num, kij\n",
    "print(W)\n",
    "W = w_tile[:,0,:,0]\n",
    "\n",
    "W_lis = []\n",
    "for out_ch in range(w_tile.size(1)):\n",
    "  W = w_tile[:,out_ch,:,0]\n",
    "  W = torch.reshape(W, (-1, W.size(0)*W.size(1)))\n",
    "  W_lis.append(W.tolist()[0])\n",
    "\n",
    "W = torch.tensor(W_lis)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12cf7fcf",
   "metadata": {
    "id": "12cf7fcf"
   },
   "outputs": [],
   "source": [
    "# ### Complete this cell ###\n",
    "# tile_id = 0\n",
    "# kij = 0\n",
    "# W = w_tile[:,:,:,kij]  # w_tile[tile_num, array col num, array row num, kij]\n",
    "# print(w_tile.size())\n",
    "# bit_precision = 4\n",
    "\n",
    "# for kij_num in range(9):\n",
    "#     W = w_tile[:,:,:,kij_num]\n",
    "#     Z = torch.reshape(W, (-1, W.size(0)*W.size(2)))\n",
    "#     print(Z)\n",
    "#     file = open('weight_mc_kij{}.txt'.format(kij_num), 'w') #write to file\n",
    "#     file.write('#col0row7[msb-lsb],col0row6[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "#     file.write('#col1row7[msb-lsb],col1row6[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "#     file.write('#................#\\n')\n",
    "#     for i in range(Z.size(0)):\n",
    "#         lis = []\n",
    "#         for j in range(Z.size(1)): # row #\n",
    "#             if(int(Z[i,j].item())>=0):\n",
    "#                 W_bin = '{0:04b}'.format(round(Z[i,j].item()))\n",
    "#             else:\n",
    "#                 W_bin = '{0:04b}'.format(round(Z[i,j].item())+16)\n",
    "#             lis.append(W_bin)\n",
    "#             #for k in range(bit_precision):\n",
    "#             #   file.write(W_bin[k])\n",
    "#             #file.write(' ')  # for visibility with blank between words, you can use\n",
    "#         print(lis)\n",
    "#         string_line = ''.join(reversed(lis))\n",
    "#         file.write(string_line)\n",
    "#         file.write('\\n')\n",
    "#     file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "qSnLOmalv86l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "qSnLOmalv86l",
    "outputId": "8b879955-e467-4fba-a133-2e7975bbb549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "### Complete this cell ###\n",
    "tile_id = 0\n",
    "kij = 0\n",
    "W = w_tile[:,:,:,1]  # w_tile[tile_num, array col num, array row num, kij]\n",
    "print(w_tile.size())\n",
    "bit_precision = 4\n",
    "\n",
    "for kij_num in range(9):\n",
    "    W_lis = []\n",
    "    for out_ch in range(w_tile.size(1)):\n",
    "      W = w_tile[:,out_ch,:,kij_num]\n",
    "      W = torch.reshape(W, (-1, W.size(0)*W.size(1)))\n",
    "      W_lis.append(W.tolist()[0])\n",
    "    Z = torch.tensor(W_lis)\n",
    "    file = open('weight_mc_kij{}.txt'.format(kij_num), 'w') #write to file\n",
    "    file.write('#col0row7[msb-lsb],col0row6[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "    file.write('#col1row7[msb-lsb],col1row6[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "    file.write('#................#\\n')\n",
    "    for i in range(Z.size(0)):\n",
    "        lis = []\n",
    "        for j in range(Z.size(1)): # row #\n",
    "            if(int(Z[i,j].item())>=0):\n",
    "                W_bin = '{0:04b}'.format(round(Z[i,j].item()))\n",
    "            else:\n",
    "                W_bin = '{0:04b}'.format(round(Z[i,j].item())+16)\n",
    "            lis.append(W_bin)\n",
    "            #for k in range(bit_precision):\n",
    "            #   file.write(W_bin[k])\n",
    "            #file.write(' ')  # for visibility with blank between words, you can use\n",
    "        string_line = ''.join(reversed(lis))\n",
    "        file.write(string_line)\n",
    "        file.write('\\n')\n",
    "    file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "endangered-reach",
   "metadata": {
    "id": "endangered-reach"
   },
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "tile_id = 0\n",
    "kij = 0\n",
    "W = w_tile[:,:,:,kij]  # w_tile[tile_num, array col num, array row num, kij]\n",
    "bit_precision = 4\n",
    "\n",
    "for kij_num in range(9):\n",
    "    W_lis = []\n",
    "    for out_ch in range(w_tile.size(1)):\n",
    "      W = w_tile[:,out_ch,:,kij_num]\n",
    "      W = torch.reshape(W, (-1, W.size(0)*W.size(1)))\n",
    "      W_lis.append(W.tolist()[0])\n",
    "    Z = torch.tensor(W_lis)\n",
    "    #print(W[:,0,:])\n",
    "    #print(Z[0,:])# w_tile[tile_num, array col num, array row num, kij]\n",
    "    file = open('weight_mc_kij{}_int.txt'.format(kij_num), 'w') #write to file\n",
    "    file.write('#col0row7[msb-lsb],col0row6[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "    file.write('#col1row7[msb-lsb],col1row6[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "    file.write('#................#\\n')\n",
    "    for i in range(Z.size(0)):\n",
    "        lis = []\n",
    "        for j in range(Z.size(1)): # row #\n",
    "            lis.append(f'{round(Z[i,j].item())},')\n",
    "        #print(lis)\n",
    "        string_line = ''.join(reversed(lis))\n",
    "        file.write(string_line)\n",
    "        file.write('\\n')\n",
    "    file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "polyphonic-sheffield",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "polyphonic-sheffield",
    "outputId": "4fefc307-f294-47dd-ce73-1571b373feff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 8])\n",
      "torch.Size([36, 8])\n",
      "torch.Size([36, 8])\n",
      "torch.Size([36, 8])\n",
      "torch.Size([36, 8])\n",
      "torch.Size([36, 8])\n",
      "torch.Size([36, 8])\n",
      "torch.Size([36, 8])\n",
      "torch.Size([36, 8])\n"
     ]
    }
   ],
   "source": [
    "### Complete this cell ###\n",
    "ic_tile_id = 0\n",
    "oc_tile_id = 0\n",
    "\n",
    "\n",
    "kij = 0\n",
    "nij = 0\n",
    "\n",
    "bit_precision = 16\n",
    "\n",
    "for kij_num in range(9):\n",
    "    nij = 0\n",
    "    psum_tile = psum[ic_tile_id,oc_tile_id,:,nij:nij+36,kij_num]\n",
    "    psum_tile = torch.transpose(psum_tile,0,1)\n",
    "    print(psum_tile.size())\n",
    "\n",
    "    file = open('psum_mc_kij{}.txt'.format(kij_num), 'w') #write to file\n",
    "    file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "    file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "    file.write('#................#\\n')\n",
    "    for i in range(psum_tile.size(0)):  # time step\n",
    "        lis = []\n",
    "        for j in range(psum_tile.size(1)): # row #\n",
    "            if(psum_tile[i,j].item() >= 0):\n",
    "                psum_tile_bin = '{0:016b}'.format(round(psum_tile[i,j].item()))\n",
    "            else:\n",
    "                psum_tile_bin = '{0:016b}'.format(round(psum_tile[i,j].item())+65536)\n",
    "            lis.append(psum_tile_bin)\n",
    "            #for k in range(bit_precision):\n",
    "            #   file.write(psum_tile_bin[k])\n",
    "            #file.write(' ')  # for visibility with blank between words, you can use\n",
    "        string_line = ''.join(reversed(lis))\n",
    "        file.write(string_line)\n",
    "        file.write('\\n')\n",
    "    file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "axjARyWI7UJP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axjARyWI7UJP",
    "outputId": "67284130-32ae-4288-96ca-5443f80bac1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 8, 36, 9])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psum[:,:,:,:,:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "seventh-pharmacy",
   "metadata": {
    "id": "seventh-pharmacy"
   },
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "ic_tile_id = 0\n",
    "oc_tile_id = 0\n",
    "\n",
    "\n",
    "kij = 0\n",
    "nij = 0\n",
    "\n",
    "bit_precision = 16\n",
    "\n",
    "for kij_num in range(9):\n",
    "    nij = 0\n",
    "    psum_tile = psum[ic_tile_id,oc_tile_id,:,nij:nij+36,kij_num]\n",
    "    psum_tile = torch.transpose(psum_tile,0,1)\n",
    "\n",
    "    file = open('psum_mc_kij{}_int.txt'.format(kij_num), 'w') #write to file\n",
    "    file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "    file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "    file.write('#................#\\n')\n",
    "    for i in range(psum_tile.size(0)):  # time step\n",
    "        lis = []\n",
    "        for j in range(psum_tile.size(1)): # row #\n",
    "             lis.append(f'{round(psum_tile[i,j].item())},')\n",
    "        string_line = ''.join(reversed(lis))\n",
    "        file.write(string_line)\n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "        file.write('\\n')\n",
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4848991b",
   "metadata": {
    "id": "4848991b"
   },
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "ic_tile_id = 0\n",
    "oc_tile_id = 0\n",
    "\n",
    "\n",
    "kij = 0\n",
    "nij = 0\n",
    "\n",
    "bit_precision = 16\n",
    "file = open('out_mc.txt', 'w') #write to file\n",
    "file.write('#out7feature0[msb-lsb],out6feature0[msb-lst],....,out0feature0[msb-lst]#\\n')\n",
    "file.write('#out7feature1[msb-lsb],out6feature1[msb-lst],....,out0feature1[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(out.size(1)):  # time step\n",
    "    for j in range(out.size(0)): # row #\n",
    "        if(out[7-j,i].item() >= 0):\n",
    "            out_bin = '{0:016b}'.format(round(out[7-j,i].item()))\n",
    "        else:\n",
    "            out_bin = '{0:016b}'.format(round(out[7-j,i].item())+65536)\n",
    "        for k in range(bit_precision):\n",
    "            file.write(out_bin[k])\n",
    "    file.write('\\n')\n",
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a641e974",
   "metadata": {
    "id": "a641e974"
   },
   "outputs": [],
   "source": [
    "\n",
    "### Complete this cell ###\n",
    "ic_tile_id = 0\n",
    "oc_tile_id = 0\n",
    "\n",
    "\n",
    "kij = 0\n",
    "nij = 0\n",
    "relu = model.features[28]\n",
    "\n",
    "\n",
    "bit_precision = 16\n",
    "file = open('out_mc_relu.txt', 'w') #write to file\n",
    "file.write('#out7feature0[msb-lsb],out6feature0[msb-lst],....,out0feature0[msb-lst]#\\n')\n",
    "file.write('#out7feature1[msb-lsb],out6feature1[msb-lst],....,out0feature1[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(out.size(1)):  # time step\n",
    "    for j in range(out.size(0)): # row #\n",
    "        out_bin = relu(torch.tensor(out[7-j,i].item()))\n",
    "        out_bin_1 = '{0:016b}'.format(round(out_bin.item()))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(out_bin_1[k])\n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "111d6829",
   "metadata": {
    "id": "111d6829"
   },
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "ic_tile_id = 0\n",
    "oc_tile_id = 0\n",
    "\n",
    "\n",
    "kij = 0\n",
    "nij = 0\n",
    "psum_tile = psum[ic_tile_id,oc_tile_id,:,nij:nij+64,kij]\n",
    "# psum[len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)]\n",
    "\n",
    "bit_precision = 16\n",
    "file = open('out_int_relu.txt', 'w') #write to file\n",
    "file.write('#out7feature0[msb-lsb],out6feature0[msb-lst],....,out0feature0[msb-lst]#\\n')\n",
    "file.write('#out7feature1[msb-lsb],out6feature1[msb-lst],....,out0feature1[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "\n",
    "for i in range(out.size(1)):  # time step\n",
    "    for j in range(out.size(0)): # row #\n",
    "        out_bin = relu(torch.tensor(out[7-j,i].item()))\n",
    "        file.write(f'{round(out_bin.item())},')\n",
    "        file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975201e",
   "metadata": {
    "id": "9975201e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
